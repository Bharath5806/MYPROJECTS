# PROJECT 1

# CI/CD Pipeline with GitHub Actions & Docker (No Cloud Needed)


Automate Code Deployment Using CI/CD Pipeline (GitHub Actions) • Objective: Set up a CI/CD pipeline to build and deploy a web app. In this project, we leverage Jenkins to create a robust CI/CD pipeline that integrates tools like Docker, and OWASP Dependency Check to deliver secure and high-quality software. Tools used:

1.GitHub

2.Jenkins

3.Docker Hub

4.Docker

5.nodeJS

6.minikube

GITHUB: GitHub is a platform for hosting and sharing code using Git. It helps developers collaborate by tracking changes, managing versions, and reviewing code. You can create repositories(repos) to store and organize projects. It also offers features like issue tracking and CI/CD workflows.

JENKINS: Jenkins is an automation tool for building and deploying software. It uses pipelines to automate tasks like testing, building, and deploying code. It supports plugins to integrate with various tools. Jenkins makes Continuous Integration and Continuous Delivery (CI/CD) easy.

DOCKER: Docker is a tool to create, share, and run lightweight virtualized environments called containers. Containers package code and dependencies together for consistent behavior across systems. It simplifies app deployment and avoids “it works on my machine” issues. Docker images are reusable blueprints for containers.

NODE.JS: Node.js is a runtime that lets you run JavaScript outside the browser. It’s great for building fast and scalable server-side applications. Node.js uses non-blocking, event-driven programming for high performance. It’s commonly used for web servers, APIs, and real-time apps.

CI/CD WORKFLOW: • The pipeline automates build, test, and deploy processes, making it efficient and reliable. • Security and quality checks are seamlessly integrated into the pipeline, ensuring every release is both secure and robust.

Minikube:

Minikube is a tool that lets you run Kubernetes locally. It creates a single-node Kubernetes cluster inside a virtual machine or container on your local system, making it perfect for developers and learners to test, experiment, and build Kubernetes applications in a safe environment.

CI/CD WORKFLOW: • The pipeline automates build, test, and deploy processes, making it efficient and reliable. • Security and quality checks are seamlessly integrated into the pipeline, ensuring every release is both secure and robust.

In Jenkins, webhooks are commonly used to trigger Jenkins jobs based on events occurring in external systems, such as GitHub, GitLab, or Bitbucket. Webhooks allow Jenkins to automatically start a build when certain events, like code pushes, pull requests, or branch updates, happen on the external system

In Jenkins, webhooks are commonly used to trigger Jenkins jobs based on events occurring in external systems, such as GitHub, GitLab, or Bitbucket. Webhooks allow Jenkins to automatically start a build when certain events, like code pushes, pull requests, or branch updates, happen on the external system.

 ## Setting up Webhooks in Jenkins:
 
Install the necessary plugins: To use webhooks, ensure you have the appropriate plugins installed in Jenkins. For Git-based systems, this typically involves:

GitHub Plugin

GitLab Plugin

Bitbucket Plugin 

These plugins handle the integration between Jenkins and the respective platform.

Create or configure a Jenkins job: You’ll need to create or configure a Jenkins job (usually a pipeline or freestyle job) that will be triggered by the webhook. This job typically contains the steps that Jenkins will execute when the event occurs.

Configure the webhook on your external system (e.g., GitHub):

For GitHub, go to the repository’s settings > Webhooks > Add webhook.
In the Payload URL, enter the Jenkins webhook URL. This will generally be something like:
~~~bash
http://<jenkins-server>/github-webhook/
~~~
Set the content type to application/json.

Choose which events should trigger the webhook (e.g., push events, pull requests, etc.).

Finally, save the webhook.

Configure Jenkins to handle the webhook: Jenkins will need to listen for incoming webhooks, and this is usually done using the GitHub Plugin or GitLab Plugin. Once you have set up your Jenkins job:

Go to the job configuration.
Under Build Triggers, enable the trigger option that corresponds to the external service, such as GitHub hook trigger for GITScm polling or similar options for GitLab or Bitbucket.
Test the webhook: Once everything is set up, push changes to the external system (e.g., commit code to GitHub). The webhook should notify Jenkins, and it should automatically trigger the configured job.

Example: GitHub Webhook in Jenkins

Install the GitHub plugin in Jenkins.

Create a Jenkins job (e.g., a Freestyle or Pipeline job).

In the Jenkins job configuration, under Build Triggers, check GitHub hook trigger for GITScm polling.

Copy the webhook URL generated by Jenkins (it should look like: http://<your-jenkins-server>/github-webhook/).

In your GitHub repository, navigate to Settings > Webhooks.

Add a new webhook, set the Payload URL to the Jenkins webhook URL, and select the events that should trigger the webhook (usually "Push events").

Save the webhook, and now Jenkins will automatically trigger a build whenever the selected event occurs on GitHub.

### Use Cases:

Continuous Integration (CI): Automatically trigger builds when new code is pushed.

Pull Request Builds: Trigger builds when a pull request is created or updated.

Branch Builds: Trigger builds when changes are made to specific branches.

By using webhooks, you can integrate Jenkins with external platforms to automatically trigger builds and streamline your CI/CD pipeline.

Explanation of the Pipeline:

Agent: Runs on any available Jenkins agent.

Environment: Defines variables like app name and Docker image tag (using build number).

 ##3 Stages:
 
Checkout: Pulls code from a Git repository (replace the URL with the actual repo).

Build: Compiles and packages the app using Maven (assumed tech stack).

Test: Runs unit tests and publishes results.

Build Docker Image: Creates a Docker image for the app.

Deploy to Staging: Runs the Docker container in a staging environment.

Approval: Manual approval step before production deployment.

Deploy to Production: Pushes the Docker image to a registry and deploys (simplified here).

Post: Actions to run after the pipeline completes (e.g., cleanup, notifications).

## Prerequisites:
Jenkins server installed with necessary plugins (e.g., Git, Maven, Docker, Pipeline).

Maven and Docker installed on the Jenkins agent.

Access to a Git repository and Docker registry.

Customization Notes:
Replace the Git URL with Zomato’s actual repository.

Adjust the build command (mvn clean package) if Zomato uses a different framework (e.g., Node.js, Python).

Update deployment steps based on Zomato’s infrastructure (e.g., Kubernetes, AWS ECS).

Add credentials for Git/Docker registry if required (use Jenkins credentials management).
![images](https://github.com/user-attachments/assets/72bbcab9-b2a7-4abc-8f05-7a763a7fd274)
![IMG_20250408_204552](https://github.com/user-attachments/assets/5a35f68f-45d7-4e52-adfa-a7998c3914c0)
![IMG_20250408_204652](https://github.com/user-attachments/assets/c03cec5e-47fb-4e4f-b7b5-6df4747874a3)
![IMG_20250408_204634](https://github.com/user-attachments/assets/700dd15e-9d79-458d-819c-ba6b42cc9b60)
![images](https://github.com/user-attachments/assets/dadef7f1-b6c2-470d-94bf-11cb5f1e5457)





# PROJECT 2

## Kubernetes-Based Canary Deployment with K3s and Istio

### Objective: 

Simulate modern canary deployments with traffic splitting between stable and new app

versions.

### Tools: 

K3s (lightweight Kubernetes), Istio, Docker, Helm, Node.js/Python app

K3s: Runs the Kubernetes cluster for app deployment.



Istio: Manages traffic splitting between stable and canary app versions.



Docker: Containers for the app (v1: stable, v2: canary).



Helm: Simplifies deployment of app and Istio configurations.



Monitoring: Istio’s telemetry (via Prometheus/Grafana) to observe traffic and performance.

Canary deployments are a strategy to reduce the risk of introducing a new software version in production by slowly rolling out the change to a small subset of users before making it available to everybody.

This technique allows developers and operations teams to monitor the new version’s performance and stability in a real-world environment with actual traffic, which helps in identifying any issues before they affect all users. Kubernetes, with its robust orchestration capabilities, provides an excellent platform for implementing canary deployments. This guide delves into the methodology, tools, and practices for executing canary deployments within a Kubernetes ecosystem.

## Why Canary Deployments?

Imagine you are managing an e-commerce platform that serves thousands of users daily. The platform is hosted on a Kubernetes cluster, and you’re planning to release a new version of the application that includes both a new user interface and several backend optimizations intended to improve page load times and checkout process efficiency.

## Benefits of Canary Deployments


Canary deployments are a strategy to reduce the risk of introducing a new software version in production by slowly rolling out the change to a small subset of users before making it available to everybody.

This technique allows developers and operations teams to monitor the new version’s performance and stability in a real-world environment with actual traffic, which helps in identifying any issues before they affect all users. Kubernetes, with its robust orchestration capabilities, provides an excellent platform for implementing canary deployments. This guide delves into the methodology, tools, and practices for executing canary deployments within a Kubernetes ecosystem.

Understanding Canary Deployments

The core idea behind a canary deployment is to deploy a new version of an application alongside the stable running version, but only direct a small percentage of the traffic to the new version. This approach is named after the “canary in a coal mine” concept, where canaries were used to provide early warning signals for dangerous conditions.


Why Canary Deployments?

Imagine you are managing an e-commerce platform that serves thousands of users daily. The platform is hosted on a Kubernetes cluster, and you’re planning to release a new version of the application that includes both a new user interface and several backend optimizations intended to improve page load times and checkout process efficiency.

Benefits of Canary Deployments

Reduced Risk: By gradually increasing the traffic to the new version, you minimize the impact of any potential issues.
Real-world Testing: Canary deployments allow you to test new features or versions under real-world conditions without impacting all your users.
Quick Rollback: If the new version exhibits any problematic behavior, you can quickly rollback to the previous version.
The changes are significant and, while tested in development and staging environments, could behave differently under the load and variety of real-world use. An immediate full-scale rollout risks impacting the user experience negatively if unforeseen issues arise, potentially leading to lost sales and damage to the company’s reputation.

## Implementing Canary Deployments in Kubernetes

Canary deployments allow for the gradual rollout of new application versions, facilitating real-world testing and risk mitigation. While Kubernetes does not offer a built-in canary deployment feature, its flexible architecture supports implementing this strategy using Deployments, Services, and Ingress controllers.

### Step 1: Preparing our Environment

Before you start, ensure that your Kubernetes cluster is operational and accessible. Install kubectl, the Kubernetes command-line tool, and configure it to communicate with your cluster. A solid understanding of Kubernetes concepts such as Deployments and Services is also crucial.

### Step 2: Deploying the Stable Version


Deploy the current stable version of our application to serve as the baseline for the canary deployment.

### Deployment and Service for Stable Version 


Canary deployments are a strategy to reduce the risk of introducing a new software version in production by slowly rolling out the change to a small subset of users before making it available to everybody.

This technique allows developers and operations teams to monitor the new version’s performance and stability in a real-world environment with actual traffic, which helps in identifying any issues before they affect all users. Kubernetes, with its robust orchestration capabilities, provides an excellent platform for implementing canary deployments. This guide delves into the methodology, tools, and practices for executing canary deployments within a Kubernetes ecosystem.

## Understanding Canary Deployments


The core idea behind a canary deployment is to deploy a new version of an application alongside the stable running version, but only direct a small percentage of the traffic to the new version. This approach is named after the “canary in a coal mine” concept, where canaries were used to provide early warning signals for dangerous conditions.

## Why Canary Deployments?


Imagine you are managing an e-commerce platform that serves thousands of users daily. The platform is hosted on a Kubernetes cluster, and you’re planning to release a new version of the application that includes both a new user interface and several backend optimizations intended to improve page load times and checkout process efficiency.

## Benefits of Canary Deployments


### Reduced Risk: 
By gradually increasing the traffic to the new version, you minimize the impact of any potential issues.
### Real-world Testing: 
Canary deployments allow you to test new features or versions under real-world conditions without impacting all your users.
### Quick Rollback: 

If the new version exhibits any problematic behavior, you can quickly rollback to the previous version.

The changes are significant and, while tested in development and staging environments, could behave differently under the load and variety of real-world use. An immediate full-scale rollout risks impacting the user experience negatively if unforeseen issues arise, potentially leading to lost sales and damage to the company’s reputation.

By employing a canary deployment strategy, we can:

### Minimize Risk: 

Gradually roll out the update to a small percentage of users. This controlled approach allows you to monitor the new version in a production environment with real traffic, without affecting all users.


### Performance Monitoring: 

Specifically, you can observe the impact of backend optimizations on actual page load times and checkout efficiency across different geographies and devices, ensuring that the improvements meet your objectives.


### User Feedback: 

Collect feedback on the new interface from the subset of users. This feedback can be invaluable in identifying UX issues that weren’t caught during internal testing.

### Quick Rollback: 

If any issues are detected during the canary phase, you can quickly rollback the changes for the affected users, significantly reducing the potential negative impact on your user base and business.
Implementing Canary Deployments in Kubernetes


Canary deployments allow for the gradual rollout of new application versions, facilitating real-world testing and risk mitigation. While Kubernetes does not offer a built-in canary deployment feature, its flexible architecture supports implementing this strategy using Deployments, Services, and Ingress controllers.

Step 1: Preparing our Environment

Before we start, ensure that your Kubernetes cluster is operational and accessible. Install kubectl, the Kubernetes command-line tool, and configure it to communicate with our cluster. A solid understanding of Kubernetes concepts such as Deployments and Services is also crucial.

Step 2: Deploying the Stable Version

Deploy the current stable version of your application to serve as the baseline for the canary deployment.

Example Deployment and Service for Stable Version

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-stable
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: stable
  template:
    metadata:
      labels:
        app: myapp
        version: stable
    spec:
      containers:
      - name: myapp
        image: myapp:stable
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: myapp
 ```

This configuration creates a Deployment named myapp-stable with three replicas and a Service that exposes the deployment.

### Step 3: Deploying the Canary Version

Introduce the new version of your application as a canary deployment with a limited number of replicas.

### Deployment for Canary Version

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-canary
spec:
  replicas: 1 # Start small
  selector:
    matchLabels:
      app: myapp
      version: canary
  template:
    metadata:
      labels:
        app: myapp
        version: canary
    spec:
      containers:
      - name: myapp
        image: myapp:canary
```

This creates a new Deployment myapp-canary for the canary version. Initially, only a small fraction of the total traffic is directed to this deployment.

### Step 4: Routing Traffic

To control traffic flow between the stable and canary versions, employ one of the following strategies:

### Service Weights with Istio

Using Istio, we can define a VirtualService to split traffic between our stable and canary deployments based on weights.

### VirtualService Configuration

```bash
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: myapp
spec:
  hosts:
  - myapp
  http:
  - route:
    - destination:
        host: myapp-stable
      weight: 80
    - destination:
        host: myapp-canary
      weight: 20
```
This configuration routes 80% of the traffic to the stable version and 20% to the canary version.

### Ingress Configuration

```bash
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
spec:
  rules:
  - http:
      paths:
      - path: /canary
        pathType: Prefix
        backend:
          service:
            name: myapp-canary
            port:
              number: 8080
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-stable
            port:
              number: 8080
```

This setup directs traffic with the path /canary to the canary version, while all other traffic goes to the stable version.

### Step 5: Monitoring and Scaling

Utilize Prometheus or another monitoring tool to observe the performance and error rates of your canary deployment. If the canary version proves stable and efficient, gradually shift more traffic from the stable version to the canary version by adjusting the weights or rules defined in Step 4.

### Step 6: Finalizing the Rollout


After thoroughly testing the canary version and confirming its stability and performance, proceed to roll out the new version across our cluster. Update the stable deployment to use the new version’s image and scale down or remove the canary deployment.

### Service Mesh for Traffic Management

Istio: Control traffic flow to our canary deployment in Istio with VirtualServices:

```bash 
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: myapp
spec:
  hosts:
    - myapp
  http:
    - route:
        - destination:
            host: myapp-stable
          weight: 80
        - destination:
            host: myapp-canary
          weight: 20
```

This YAML snippet directs 20% of traffic to the canary and 80% to the stable version.

### Understanding what happened 

Stage 1:

K8S_CANARY_ROLLOUT ensures that the workloads of canary variant (new version) should be deployed. But at this time, they still handle nothing, all traffic are handled by workloads of primary variant. The number of workloads for canary variant is configured to be 50% of the replicas number of primary varant.

![example-canary-kubernetes-istio-stage-1](https://github.com/user-attachments/assets/28e3dd23-381a-4482-b9a6-24605cba1f29)

Stage 2: K8S_TRAFFIC_ROUTING ensures that 20% of traffic should be routed to canary variant and 80% to primary variant. Because the trafficRouting is configured to use Istio, PipeCD will find Istio’s VirtualService resource of this application to control the traffic percentage.

![IMG_20250425_171420](https://github.com/user-attachments/assets/09ce6287-09ef-4000-8009-04330894cf81)

Stage 3: WAIT_APPROVAL waits for a manual approval from someone in your team.

Stage 4: K8S_PRIMARY_ROLLOUT ensures that all resources of primary variant will be updated to the new version.

![IMG_20250425_171420](https://github.com/user-attachments/assets/a5a05a6b-7eab-4747-bad6-e13d597944bc)


Stage 5: K8S_TRAFFIC_ROUTING ensures that all traffic should be routed to primary variant. Now primary variant is running the new version so it means all traffic is handled by the new version.

![example-canary-kubernetes-istio-stage-1](https://github.com/user-attachments/assets/cad4e208-5007-4ddb-854e-7abc56d18175)

Stage 6: K8S_CANARY_CLEAN ensures all created resources for canary variant should be destroyed.


![example-canary-kubernetes-istio-stage-1](https://github.com/user-attachments/assets/6e6e249b-b3b5-4afc-a8a3-8e89783fbff9)


